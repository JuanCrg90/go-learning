## ğŸš€ Project: Concurrent Web Scraper

### **ğŸ¯ Objective**

Build a CLI Go program that:

* Fetches content from multiple URLs concurrently
* Measures response times
* Prints results in a clean, readable format

---

## âœ… Suggested Features

| Feature               | Description                                  |
| --------------------- | -------------------------------------------- |
| ğŸ§µ Goroutines         | Launch a goroutine per URL                   |
| ğŸ”— Channels           | Collect results (URL, status code, duration) |
| âŒ› WaitGroup           | Wait for all requests to complete            |
| â±ï¸ Context (optional) | Add timeout for slow URLs                    |
| ğŸ“Š Output             | Show status and time per site in console     |

---

## ğŸ“¦ Suggested Project Structure

```
web-scraper/
â”œâ”€â”€ main.go
â”œâ”€â”€ fetcher.go   # logic to fetch URLs
â”œâ”€â”€ types.go     # result structs (optional)
```

---

## ğŸ§ª Starter URLs

Use a list like:

```go
urls := []string{
	"https://golang.org",
	"https://httpbin.org/delay/2",
	"https://example.com",
}
```

---

## âœ¨ Bonus Ideas (Optional Enhancements)

* Retry failed requests
* Save output as JSON or CSV
* Use a context to enforce global timeout
* Color-code fast/slow responses in output
